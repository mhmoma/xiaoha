# -*- coding: utf-8 -*-
"""
åˆ†ç±»è¯åº“æ–‡ä»¶ä¸­çš„è¯æ¡ï¼Œç„¶åä¸knowledge_base.jsonåˆå¹¶
"""
import json
import os
import re

# åˆ†ç±»è§„åˆ™ï¼šåŸºäºå…³é”®è¯åŒ¹é…
CLASSIFICATION_RULES = {
    "Body Parts": [
        r'\b(ear|eye|nose|mouth|lip|tongue|neck|shoulder|arm|hand|finger|leg|foot|toe|breast|nipple|areola|butt|thigh|knee|ankle|wrist|elbow|hip|waist|chest|back|stomach|belly|abs|muscle|bone|skeleton)\b',
        r'\b(long|short|large|small|big|tiny|thick|thin)\s+(ear|eye|nose|mouth|lip|tongue|neck|arm|hand|leg|foot|breast|butt|thigh)\b',
    ],
    "Eyes": [
        r'\b(eye|pupil|iris|eyelid|eyelash|eyebrow)\b',
        r'\b(blue|green|brown|red|yellow|purple|pink|black|white|gray|grey)\s+eye\b',
        r'\b(heterochromia|monoeye|closed\s+eye|open\s+eye|wide\s+eye)\b',
    ],
    "Hair Color & Style": [
        r'\b(hair|ponytail|braid|twintail|bun|bangs|fringe|curly|straight|wavy|spiky|messy|neat|long|short|medium)\s+hair\b',
        r'\b(blue|green|brown|red|yellow|purple|pink|black|white|gray|grey|blonde|silver|golden|auburn|chestnut)\s+hair\b',
        r'\b(hair\s+ornament|hair\s+ribbon|hair\s+clip|hair\s+band|hair\s+bow)\b',
    ],
    "Facial Expressions": [
        r'\b(smile|frown|grin|pout|blush|tear|cry|laugh|wink|stare|glare|surprised|shocked|angry|sad|happy|joy|sadness|fear|disgust|surprise)\b',
        r'\b(open\s+mouth|closed\s+mouth|tongue\s+out|licking\s+lips)\b',
    ],
    "Posture": [
        r'\b(standing|sitting|lying|kneeling|crouching|jumping|running|walking|dancing|posing)\b',
        r'\b(looking\s+at\s+viewer|looking\s+away|looking\s+up|looking\s+down|looking\s+back)\b',
        r'\b(arms\s+up|arms\s+behind|hands\s+on\s+hip|hands\s+behind\s+head|crossed\s+arms)\b',
    ],
    "Topwear": [
        r'\b(shirt|blouse|top|tank\s+top|crop\s+top|t-shirt|tshirt|sweater|hoodie|jacket|coat|vest|bra|underwear)\b',
        r'\b(long|short|sleeveless)\s+sleeve\b',
    ],
    "Bottomwear": [
        r'\b(pants|trousers|jeans|shorts|skirt|miniskirt|long\s+skirt)\b',
    ],
    "Dresses": [
        r'\b(dress|gown|kimono|yukata|qipao|cheongsam)\b',
    ],
    "Footwear": [
        r'\b(shoes|boots|sneakers|sandals|high\s+heels|heels|slippers|barefoot)\b',
    ],
    "Headwear": [
        r'\b(hat|cap|beanie|helmet|crown|tiara|headband|hairband|ribbon|bow)\b',
    ],
    "Locations": [
        r'\b(background|indoor|outdoor|room|bedroom|bathroom|kitchen|street|park|beach|forest|mountain|city|building|house|school|office)\b',
        r'\b(simple|white|black|gradient|nature|urban|rural)\s+background\b',
    ],
    "Format": [
        r'\b(highres|absurdres|lowres|quality|masterpiece|best\s+quality|worst\s+quality)\b',
        r'\b(1girl|1boy|2girls|multiple\s+girls|solo|group)\b',
    ],
    "View Angle": [
        r'\b(from\s+above|from\s+below|from\s+side|from\s+behind|from\s+front|bird\s+eye|worm\s+eye)\b',
        r'\b(close-up|closeup|full\s+body|upper\s+body|lower\s+body|head\s+only)\b',
    ],
    "Styles and Techniques": [
        r'\b(anime|manga|realistic|photorealistic|sketch|watercolor|oil\s+painting|digital\s+art|pixel\s+art|3d|2d)\b',
        r'\b(cel\s+shading|soft\s+shading|hard\s+shading|no\s+shading)\b',
    ],
    "Breasts": [
        r'\b(breast|breasts|chest|cleavage|underboob|sideboob|flat\s+chest|large\s+breasts|small\s+breasts|medium\s+breasts)\b',
    ],
    "Sleeves": [
        r'\b(long\s+sleeves|short\s+sleeves|sleeveless|detached\s+sleeves|puffy\s+sleeves)\b',
    ],
    "Neckwear": [
        r'\b(necklace|choker|scarf|tie|bow\s+tie|necktie)\b',
    ],
    "Wings": [
        r'\b(wing|wings|angel\s+wing|demon\s+wing|butterfly\s+wing)\b',
    ],
    "Tails": [
        r'\b(tail|tails|fox\s+tail|cat\s+tail|dog\s+tail|bunny\s+tail)\b',
    ],
    "Focus": [
        r'\b(focus|blur|bokeh|depth\s+of\s+field|shallow\s+focus)\b',
    ],
    "Swimsuits and Bodysuits": [
        r'\b(swimsuit|bikini|one\s+piece|bodysuit|leotard)\b',
    ],
    "Full Body Outfits": [
        r'\b(uniform|school\s+uniform|maid\s+uniform|nurse\s+uniform|sailor\s+uniform)\b',
        r'\b(armor|suit|tuxedo|wedding\s+dress)\b',
    ],
    "Sexual Attire": [
        r'\b(lingerie|bra|panties|underwear|nude|naked|topless|bottomless)\b',
    ],
    "Sexual Positions": [
        r'\b(cowgirl|missionary|doggy|69|blowjob|handjob)\b',
    ],
    "Sex Acts": [
        r'\b(sex|intercourse|penetration|oral|anal|vaginal)\b',
    ],
}

# æœªåˆ†ç±»çš„è¯æ¡å°†æ”¾å…¥æ­¤åˆ†ç±»
UNCLASSIFIED_CATEGORY = "Unclassified"

# é¢„ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼ä»¥æé«˜æ€§èƒ½
COMPILED_PATTERNS = {}
for category, patterns in CLASSIFICATION_RULES.items():
    COMPILED_PATTERNS[category] = [re.compile(pattern, re.IGNORECASE) for pattern in patterns]

def classify_term(term):
    """
    æ ¹æ®è¯æ¡å†…å®¹åˆ†ç±»
    è¿”å›åˆ†ç±»åç§°åˆ—è¡¨ï¼ˆä¸€ä¸ªè¯æ¡å¯èƒ½å±äºå¤šä¸ªåˆ†ç±»ï¼‰
    """
    term_lower = term.lower()
    categories = []
    
    for category, compiled_patterns in COMPILED_PATTERNS.items():
        for pattern in compiled_patterns:
            if pattern.search(term_lower):
                if category not in categories:
                    categories.append(category)
                break  # æ‰¾åˆ°ä¸€ä¸ªåŒ¹é…å°±è¶³å¤Ÿäº†
    
    return categories if categories else [UNCLASSIFIED_CATEGORY]

def classify_lexicon(lexicon_data):
    """
    å¯¹è¯åº“è¿›è¡Œåˆ†ç±»
    """
    classified_data = {}
    
    # åˆå§‹åŒ–æ‰€æœ‰åˆ†ç±»
    for category in CLASSIFICATION_RULES.keys():
        classified_data[category] = []
    classified_data[UNCLASSIFIED_CATEGORY] = []
    
    print("ğŸ“š å¼€å§‹åˆ†ç±»è¯æ¡...")
    
    # éå†æ‰€æœ‰è¯æ¡
    total_items = 0
    for category_name, items in lexicon_data.items():
        print(f"   å¤„ç†åˆ†ç±»: {category_name} ({len(items)} ä¸ªè¯æ¡)...")
        total_items += len(items)
        
        for i, item in enumerate(items):
            term = item.get('term', '').strip()
            if not term:
                continue
            
            # åˆ†ç±»
            categories = classify_term(term)
            
            # æ·»åŠ åˆ°å¯¹åº”åˆ†ç±»ï¼ˆä¸€ä¸ªè¯æ¡å¯èƒ½å±äºå¤šä¸ªåˆ†ç±»ï¼‰
            for cat in categories:
                if cat not in classified_data:
                    classified_data[cat] = []
                
                # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ï¼ˆé¿å…é‡å¤ï¼‰
                existing_terms = {item['term'] for item in classified_data[cat]}
                if term not in existing_terms:
                    classified_data[cat].append({
                        'term': term,
                        'translation': item.get('translation', '').strip()
                    })
            
            # æ˜¾ç¤ºè¿›åº¦
            if (i + 1) % 10000 == 0:
                print(f"     å·²å¤„ç†: {i + 1}/{len(items)} ({((i+1)/len(items)*100):.1f}%)")
    
    print(f"\nâœ… åˆ†ç±»å®Œæˆï¼å…±å¤„ç† {total_items} ä¸ªè¯æ¡")
    
    # ç»Ÿè®¡ä¿¡æ¯
    print(f"\nğŸ“Š åˆ†ç±»ç»Ÿè®¡:")
    for category, items in sorted(classified_data.items(), key=lambda x: len(x[1]), reverse=True):
        if items:  # åªæ˜¾ç¤ºæœ‰å†…å®¹çš„åˆ†ç±»
            print(f"   - {category}: {len(items)} ä¸ªè¯æ¡")
    
    return classified_data

def merge_knowledge_bases(classified_data, kb_data):
    """
    åˆå¹¶åˆ†ç±»åçš„è¯åº“å’Œknowledge_base.json
    """
    print("\nğŸ”„ å¼€å§‹åˆå¹¶çŸ¥è¯†åº“...")
    
    merged_data = {}
    
    # å…ˆæ·»åŠ knowledge_base.jsonçš„æ‰€æœ‰åˆ†ç±»ï¼ˆä¼˜å…ˆçº§æ›´é«˜ï¼‰
    for category, items in kb_data.items():
        merged_data[category] = items
        print(f"   âœ“ æ·»åŠ åˆ†ç±»: {category} ({len(items)} ä¸ªè¯æ¡)")
    
    # æ·»åŠ åˆ†ç±»åçš„è¯åº“
    for category, items in classified_data.items():
        if not items:  # è·³è¿‡ç©ºåˆ†ç±»
            continue
            
        if category in merged_data:
            # åˆå¹¶å»é‡
            print(f"   âš  åˆ†ç±» '{category}' å·²å­˜åœ¨ï¼Œæ­£åœ¨åˆå¹¶å»é‡...")
            existing_terms = {item['term']: item for item in merged_data[category]}
            new_count = 0
            for item in items:
                term = item.get('term', '').strip()
                if term and term not in existing_terms:
                    existing_terms[term] = item
                    new_count += 1
            merged_data[category] = list(existing_terms.values())
            print(f"     æ·»åŠ äº† {new_count} ä¸ªæ–°è¯æ¡ï¼Œæ€»è®¡ {len(merged_data[category])} ä¸ªè¯æ¡")
        else:
            # æ–°åˆ†ç±»ï¼Œç›´æ¥æ·»åŠ 
            merged_data[category] = items
            print(f"   âœ“ æ·»åŠ åˆ†ç±»: {category} ({len(items)} ä¸ªè¯æ¡)")
    
    return merged_data

def main():
    """
    ä¸»å‡½æ•°ï¼šåˆ†ç±»è¯åº“å¹¶åˆå¹¶
    """
    try:
        script_dir = os.path.dirname(os.path.abspath(__file__))
        os.chdir(script_dir)
        
        lexicon_file = os.path.join(script_dir, 'è¯åº“.json')
        kb_file = os.path.join(script_dir, 'knowledge_base.json')
        classified_file = os.path.join(script_dir, 'classified_lexicon.json')
        merged_file = os.path.join(script_dir, 'merged_knowledge_base.json')
        
        print("=" * 60)
        print("  è¯åº“åˆ†ç±»ä¸åˆå¹¶å·¥å…·")
        print("=" * 60)
        print()
        
        # æ­¥éª¤1: è¯»å–è¯åº“æ–‡ä»¶
        print("ğŸ“– æ­¥éª¤1: è¯»å–è¯åº“æ–‡ä»¶...")
        if not os.path.exists(lexicon_file):
            print(f"âŒ é”™è¯¯: æ–‡ä»¶ä¸å­˜åœ¨: {lexicon_file}")
            return
        
        with open(lexicon_file, 'r', encoding='utf-8') as f:
            lexicon_data = json.load(f)
        print(f"âœ… å·²è¯»å–: {lexicon_file}")
        print()
        
        # æ­¥éª¤2: åˆ†ç±»è¯æ¡
        print("ğŸ“š æ­¥éª¤2: åˆ†ç±»è¯æ¡...")
        classified_data = classify_lexicon(lexicon_data)
        print()
        
        # ä¿å­˜åˆ†ç±»åçš„è¯åº“
        print("ğŸ’¾ ä¿å­˜åˆ†ç±»åçš„è¯åº“...")
        with open(classified_file, 'w', encoding='utf-8') as f:
            json.dump(classified_data, f, ensure_ascii=False, indent=2)
        print(f"âœ… å·²ä¿å­˜: {classified_file}")
        print()
        
        # æ­¥éª¤3: è¯»å–knowledge_base.json
        print("ğŸ“– æ­¥éª¤3: è¯»å–knowledge_base.json...")
        kb_data = {}
        if os.path.exists(kb_file):
            with open(kb_file, 'r', encoding='utf-8') as f:
                kb_data = json.load(f)
            print(f"âœ… å·²è¯»å–: {kb_file}")
        else:
            print(f"âš ï¸  æ–‡ä»¶ä¸å­˜åœ¨: {kb_file}ï¼Œå°†åªä½¿ç”¨åˆ†ç±»åçš„è¯åº“")
        print()
        
        # æ­¥éª¤4: åˆå¹¶çŸ¥è¯†åº“
        print("ğŸ”„ æ­¥éª¤4: åˆå¹¶çŸ¥è¯†åº“...")
        merged_data = merge_knowledge_bases(classified_data, kb_data)
        print()
        
        # æ­¥éª¤5: ä¿å­˜åˆå¹¶åçš„çŸ¥è¯†åº“
        print("ğŸ’¾ æ­¥éª¤5: ä¿å­˜åˆå¹¶åçš„çŸ¥è¯†åº“...")
        with open(merged_file, 'w', encoding='utf-8') as f:
            json.dump(merged_data, f, ensure_ascii=False, indent=2)
        print(f"âœ… å·²ä¿å­˜: {merged_file}")
        print()
        
        # æœ€ç»ˆç»Ÿè®¡
        print("=" * 60)
        print("âœ… å¤„ç†å®Œæˆï¼")
        print("=" * 60)
        print(f"ğŸ“Š æœ€ç»ˆç»Ÿè®¡:")
        print(f"   - åˆ†ç±»æ•°é‡: {len(merged_data)}")
        total_items = sum(len(items) for items in merged_data.values())
        print(f"   - æ€»è¯æ¡æ•°: {total_items}")
        print()
        print(f"ğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
        print(f"   - {classified_file} (åˆ†ç±»åçš„è¯åº“)")
        print(f"   - {merged_file} (åˆå¹¶åçš„çŸ¥è¯†åº“)")
        print()
        
    except FileNotFoundError as e:
        print(f"âŒ é”™è¯¯: æ–‡ä»¶æœªæ‰¾åˆ° - {e}")
    except json.JSONDecodeError as e:
        print(f"âŒ JSONè§£æé”™è¯¯: {e}")
    except Exception as e:
        print(f"âŒ å¤„ç†æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()

