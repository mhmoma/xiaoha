import json
import os
from collections import defaultdict

# --- åˆ†ç±»è§„åˆ™å®šä¹‰ ---
# æ‚¨å¯ä»¥æ ¹æ®éœ€è¦éšæ—¶ä¿®æ”¹æˆ–æ·»åŠ è¿™é‡Œçš„è§„åˆ™ã€‚
# é”®æ˜¯æ–°çš„ä¸­æ–‡ç›®å½•åï¼Œå€¼æ˜¯ç”¨äºåŒ¹é…çš„è‹±æ–‡å…³é”®è¯åˆ—è¡¨ã€‚
CLASSIFICATION_RULES = {
    "è„¸éƒ¨/è¡¨æƒ…": ["face", "expression", "eyes", "mouth", "nose", "blush", "smile", "frown", "tears", "wink", "sad", "happy"],
    "è€³æœµ": ["ears", "animal ears", "elf ears", "fox ears", "cat ears"],
    "èˆŒå¤´": ["tongue", "tongue out"],
    "å¤´å‘": ["hair", "hairstyle", "bangs", "ponytail", "twintails", "blonde hair", "brown hair", "black hair", "red hair"],
    "èº«ä½“éƒ¨ä½": ["hands", "legs", "feet", "breasts", "ass", "navel", "thighs", "armpits", "belly"],
    "æœè£…/é¥°å“": ["dress", "skirt", "shirt", "pants", "bikini", "uniform", "hat", "shoes", "gloves", "ribbon", "jewelry", "necklace"],
    "èƒŒæ™¯/ç¯å¢ƒ": ["outdoors", "indoors", "sky", "city", "forest", "beach", "water", "room", "street"],
    "åŠ¨ä½œ/å§¿åŠ¿": ["standing", "sitting", "lying", "looking at viewer", "posing", "dancing", "stretching", "holding"],
    "é£æ ¼/æ•ˆæœ": ["monochrome", "realistic", "sketch", "lineart", "blur", "cinematic", "glowing"],
    "æ‘„åƒæœº/æ„å›¾": ["from behind", "from above", "from below", "close-up", "full body", "wide shot", "cowboy shot"],
}

SOURCE_FILE = 'merged_knowledge_base.json'
OUTPUT_FILE = 'classified_lexicon.json'

def classify_lexicon():
    """
    è¯»å–åˆå¹¶åçš„çŸ¥è¯†åº“ï¼Œå¹¶æ ¹æ®è§„åˆ™è¿›è¡Œæ›´è¯¦ç»†çš„åˆ†ç±»ã€‚
    """
    if not os.path.exists(SOURCE_FILE):
        print(f"âŒ é”™è¯¯ï¼šæºæ–‡ä»¶ '{SOURCE_FILE}' ä¸å­˜åœ¨ã€‚è¯·å…ˆç¡®ä¿æœºå™¨äººå·²è¿è¡Œå¹¶ç”Ÿæˆè¯¥æ–‡ä»¶ã€‚")
        return

    print(f"ğŸ“– æ­£åœ¨è¯»å–æºçŸ¥è¯†åº“: {SOURCE_FILE}")
    with open(SOURCE_FILE, 'r', encoding='utf-8') as f:
        source_data = json.load(f)

    # ä½¿ç”¨ defaultdict æ–¹ä¾¿åœ°æ·»åŠ æ–°åˆ†ç±»
    classified_data = defaultdict(list)
    
    # åˆ›å»ºä¸€ä¸ªé›†åˆæ¥è·Ÿè¸ªå·²ç»å¤„ç†è¿‡çš„è¯æ¡ï¼Œä»¥é¿å…é‡å¤
    processed_terms = set()

    print("âš™ï¸ æ­£åœ¨è¿›è¡Œåˆ†ç±»...")

    # å°†æ‰€æœ‰æ ‡ç­¾æ‰å¹³åŒ–åˆ°ä¸€ä¸ªåˆ—è¡¨ä¸­ï¼Œå¹¶å»é‡
    all_tags = []
    for category in source_data.values():
        for tag in category:
            term = tag.get('term')
            if term and term.lower() not in processed_terms:
                all_tags.append(tag)
                processed_terms.add(term.lower())
    
    print(f"å»é‡åå…±æ‰¾åˆ° {len(all_tags)} ä¸ªç‹¬ç«‹æ ‡ç­¾ã€‚")

    # å¯¹æ‰€æœ‰æ ‡ç­¾è¿›è¡Œåˆ†ç±»
    for tag in all_tags:
        term_lower = tag.get('term', '').lower()
        assigned = False
        for category, keywords in CLASSIFICATION_RULES.items():
            for keyword in keywords:
                # ä¸ºäº†æ›´ç²¾ç¡®çš„åŒ¹é…ï¼Œæˆ‘ä»¬æ£€æŸ¥å…³é”®è¯æ˜¯å¦æ˜¯æ ‡ç­¾çš„ä¸€éƒ¨åˆ†
                # ä¾‹å¦‚ 'hair' ä¼šåŒ¹é… 'long hair'
                if f" {keyword} " in f" {term_lower} " or term_lower.startswith(keyword) or term_lower.endswith(keyword):
                    classified_data[category].append(tag)
                    assigned = True
                    break  # æ‰¾åˆ°åˆ†ç±»åï¼Œè·³å‡ºå†…éƒ¨å…³é”®è¯å¾ªç¯
            if assigned:
                break  # è·³å‡ºå¤–éƒ¨è‡ªåˆ†ç±»å¾ªç¯
        
        if not assigned:
            classified_data["æœªåˆ†ç±»"].append(tag)

    print(f"ğŸ’¾ æ­£åœ¨å°†åˆ†ç±»ç»“æœå†™å…¥: {OUTPUT_FILE}")
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:
        json.dump(classified_data, f, ensure_ascii=False, indent=2)

    print("\nğŸ‰ åˆ†ç±»å®Œæˆï¼ ğŸ‰")
    print("="*30)
    print("ğŸ“Š åˆ†ç±»ç»Ÿè®¡:")
    for category, tags in classified_data.items():
        print(f"  - {category}: {len(tags)} ä¸ªæ ‡ç­¾")
    print("="*30)

if __name__ == "__main__":
    classify_lexicon()
